# RLHF Base Repo (Text Policy) â€” Electronics/Photonics/Biomed

This repository gives you a **ready-to-run** pipeline to:
- Fine-tune a base LLM with **SFT**,
- Train a **Reward Model (RM)** on human or AI feedback,
- Optimize the policy with **PPO** (classic RLHF) or **DPO** (pairwise preference without explicit RM),
- **Evaluate** via win-rate tournaments and **safety checks**,
- **Bootstrap** your domain with tailored **prompt packs** (electronics, photonics, biomedical).

> Tested with Python 3.10+. Youâ€™ll need a CUDA GPU for training.

## ðŸš€ NEW: AutoML for Semiconductor Manufacturing

**Automated Machine Learning capabilities now available!**

This repository now includes comprehensive AutoML features specifically designed for semiconductor manufacturing processes:

### AutoML Features
- ðŸ¤– **Auto Model Selection** - Automatically find the best algorithm for your data
- ðŸŽ¯ **Hyperparameter Tuning** - Optimize model parameters automatically using Bayesian optimization
- ðŸ—ï¸ **Neural Architecture Search** - Design optimal neural networks automatically

### Quick Start with AutoML

```bash
# Run quick AutoML pipeline (5-10 minutes)
python src/automl/train_automl.py --config configs/automl/automl_quickstart.yaml

# Run full AutoML pipeline (30-60 minutes)
python src/automl/train_automl.py --config configs/automl/automl_full.yaml
```

**ðŸ“– See [AUTOML_README.md](AUTOML_README.md) for complete AutoML documentation**


---

## Quickstart

```bash
# 0) Create env
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip

# 1) Install deps
pip install -r requirements.txt

# 2) (Optional) Login to HF if you want to push models
# huggingface-cli login

# 3) Prepare a small dummy dataset (creates toy JSONLs)
python scripts/prepare_data.py

# 4) Run SFT
python src/train_sft.py --config configs/sft.yaml

# 5) Train Reward Model (pairwise preferences)
python src/train_rm.py --config configs/rlhf.yaml

# 6) PPO RLHF (uses the trained RM)
python src/train_ppo.py --config configs/rlhf.yaml

# (Alternative) DPO (no explicit RM required)
python src/train_dpo.py --config configs/rlhf.yaml

# 7) Evaluate (win-rate & safety)
python src/eval/eval.py --config configs/eval.yaml
```

### Minimal data shapes

- **SFT**: `{"prompt": "...", "response": "..."}` per line (JSONL).
- **Preferences**: `{"prompt": "...", "chosen": "...", "rejected": "..."}` per line (JSONL).

See `src/data/schemas/` for JSON Schemas and labeling guides.

---

## Repo layout

```
.
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ sft.yaml
â”‚   â”œâ”€â”€ rlhf.yaml
â”‚   â””â”€â”€ eval.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_sft.py
â”‚   â”œâ”€â”€ train_rm.py
â”‚   â”œâ”€â”€ train_ppo.py
â”‚   â”œâ”€â”€ train_dpo.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”‚   â”œâ”€â”€ reward_utils.py
â”‚   â”‚   â”œâ”€â”€ logging_utils.py
â”‚   â”‚   â”œâ”€â”€ safety_policies.py
â”‚   â”‚   â””â”€â”€ eval_metrics.py
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ eval.py
â”‚   â”‚   â”œâ”€â”€ winrate_tournament.py
â”‚   â”‚   â”œâ”€â”€ safety_checks.py
â”‚   â”‚   â””â”€â”€ adversarial_prompts.txt
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ system_electronics.txt
â”‚   â”‚   â”œâ”€â”€ system_photonics.txt
â”‚   â”‚   â”œâ”€â”€ system_biomed.txt
â”‚   â”‚   â””â”€â”€ reward_rubric.md
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ schemas/
â”‚           â”œâ”€â”€ preference_pair.schema.json
â”‚           â”œâ”€â”€ sft_example.jsonl
â”‚           â”œâ”€â”€ prefs_example.jsonl
â”‚           â”œâ”€â”€ human_labeling_guidelines.md
â”‚           â””â”€â”€ aif_judges_guidelines.md
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ sample_prompts.jsonl
â”‚   â””â”€â”€ generate_judgments_aif.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â””â”€â”€ .gitignore
```

---

## Domain-tailored prompting

We include **system prompts** to bias the model toward **electronics**, **photonics**, and **biomed** expertise. You can mix them in SFT and RLHF by prepending the appropriate `system_*` file to each prompt.

---

## Safety & governance

- Rule-based **refusals** for disallowed or high-risk requests.
- **Hallucination guards** (ask-for-citation, uncertainty prompts).
- Adversarial prompts for red-teaming.
- Metrics: refusal precision/recall, harmful content rate, jailbreak rate.

---

## License

MIT (for this scaffold). Verify licenses of any datasets/models you train.


## Multi-GPU with DeepSpeed or FSDP

This repo ships ready-made configs for **DeepSpeed ZeRO-3** (`configs/ds_zero3.json`) and **PyTorch FSDP** (`configs/fsdp_config.json`).

### DeepSpeed
- Already enabled in `configs/sft.yaml` via `deepspeed: configs/ds_zero3.json`.
- Launch example (2 GPUs):
```bash
accelerate launch --num_processes=2 src/train_sft.py --config configs/sft.yaml
```

### FSDP
- Comment out `deepspeed` and set:
```yaml
fsdp: full_shard auto_wrap
fsdp_config: configs/fsdp_config.json
```
- Then launch with Accelerate (2 GPUs):
```bash
accelerate launch --num_processes=2 src/train_sft.py --config configs/sft.yaml
```

> Tip: You can use `accelerate config` once to create a default multi-GPU config. PPO/DPO can also be launched via `accelerate`.
